srun: error: ioctl(TIOCGWINSZ): Inappropriate ioctl for device
srun: error: Not using a pseudo-terminal, disregarding --pty option
[ERROR] File already exists: /raid/local/containers/enroot-data/user-1787/pytorch
Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   6%|▌         | 1/17 [00:06<01:51,  6.99s/it]Loading checkpoint shards:  12%|█▏        | 2/17 [00:07<00:50,  3.36s/it]Loading checkpoint shards:  18%|█▊        | 3/17 [00:08<00:30,  2.18s/it]Loading checkpoint shards:  24%|██▎       | 4/17 [00:09<00:21,  1.63s/it]Loading checkpoint shards:  29%|██▉       | 5/17 [00:10<00:16,  1.34s/it]Loading checkpoint shards:  35%|███▌      | 6/17 [00:10<00:12,  1.14s/it]Loading checkpoint shards:  41%|████      | 7/17 [00:11<00:10,  1.01s/it]Loading checkpoint shards:  47%|████▋     | 8/17 [00:12<00:08,  1.08it/s]Loading checkpoint shards:  53%|█████▎    | 9/17 [00:13<00:06,  1.14it/s]Loading checkpoint shards:  59%|█████▉    | 10/17 [00:13<00:05,  1.21it/s]Loading checkpoint shards:  65%|██████▍   | 11/17 [00:14<00:04,  1.24it/s]Loading checkpoint shards:  71%|███████   | 12/17 [00:15<00:03,  1.28it/s]Loading checkpoint shards:  76%|███████▋  | 13/17 [00:16<00:03,  1.30it/s]Loading checkpoint shards:  82%|████████▏ | 14/17 [00:16<00:02,  1.30it/s]Loading checkpoint shards:  88%|████████▊ | 15/17 [00:17<00:01,  1.32it/s]Loading checkpoint shards:  94%|█████████▍| 16/17 [00:18<00:00,  1.33it/s]Loading checkpoint shards: 100%|██████████| 17/17 [00:18<00:00,  1.43it/s]Loading checkpoint shards: 100%|██████████| 17/17 [00:18<00:00,  1.12s/it]
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 226.88 MiB is free. Including non-PyTorch memory, this process has 78.88 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 395.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 174.88 MiB is free. Including non-PyTorch memory, this process has 78.93 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 447.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 198.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 423.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 168.88 MiB is free. Including non-PyTorch memory, this process has 78.93 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 453.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 192.88 MiB is free. Including non-PyTorch memory, this process has 78.91 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 429.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 162.88 MiB is free. Including non-PyTorch memory, this process has 78.94 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 459.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 190.88 MiB is free. Including non-PyTorch memory, this process has 78.91 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 431.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 158.88 MiB is free. Including non-PyTorch memory, this process has 78.94 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 463.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 188.88 MiB is free. Including non-PyTorch memory, this process has 78.91 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 433.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 156.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 465.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 186.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 435.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 186.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 435.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 186.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 435.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 186.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 435.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 186.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 435.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 254.88 MiB is free. Including non-PyTorch memory, this process has 78.85 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 367.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 24.88 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 327.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 206.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 415.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 254.88 MiB is free. Including non-PyTorch memory, this process has 78.85 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 367.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 24.88 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 327.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 206.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 415.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/projects/fraudagent/hase-michael/V3/finetune.py", line 1301, in main
    optimizer.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 90, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 772, in adamw
    func(
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
srun: error: klb-dgx-002: task 0: Exited with exit code 1
