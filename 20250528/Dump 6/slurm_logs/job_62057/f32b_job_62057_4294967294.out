
=============
== PyTorch ==
=============

NVIDIA Release 24.08 (build 107063150)
PyTorch Version 2.5.0a0+872d972
Container image Copyright (c) 2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
Copyright (c) 2014-2024 Facebook Inc.
Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)
Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)
Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)
Copyright (c) 2011-2013 NYU                      (Clement Farabet)
Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)
Copyright (c) 2006      Idiap Research Institute (Samy Bengio)
Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)
Copyright (c) 2015      Google Inc.
Copyright (c) 2015      Yangqing Jia
Copyright (c) 2013-2016 The Caffe contributors
All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

ERROR: This container was built for NVIDIA Driver Release 560.35 or later, but
       version 535.161.08 was detected and compatibility mode is UNAVAILABLE.

       [[]]

Looking in links: /cm/shared/python_modules/
Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.51.3)
Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.7.0)
Requirement already satisfied: deepspeed in /usr/local/lib/python3.10/dist-packages (0.16.0)
Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)
Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.31.4)
Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.4)
Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)
Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.7.24)
Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)
Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.1)
Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.3)
Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)
Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.0)
Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.0a0+872d972e41.nv24.8)
Requirement already satisfied: hjson in /usr/local/lib/python3.10/dist-packages (from deepspeed) (3.1.0)
Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.0.8)
Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.11.1.1)
Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)
Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.8.2)
Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.10/dist-packages (from deepspeed) (12.560.30)
Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)
Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)
Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (2.20.1)
Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.3)
Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.4)
Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)
Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)
[2025-05-26 14:45:09,971] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/users/1787/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Configuration loaded from config.json
✓ Configuration validation passed
============================================================
LOADED CONFIGURATION
============================================================
📁 Model Configuration:
  Model path: /cm/shared/llm_models/Qwen/Qwen2.5-32B-Instruct
  Model family: qwen

📊 Data Configuration:
  Training data: dataset/train_set.csv
  Evaluation data: dataset/test_set.csv

🔧 LoRA Configuration:
  Use LoRA: False

🚀 Training Configuration:
  Batch size: 1
  Learning rate: 3e-05
  Number of epochs: 3
  Max sequence length: 512
  Gradient accumulation steps: 4
  Warmup steps: 10

💾 Output Configuration:
  Output directory: Auto-generated
============================================================

🖥️  Available GPUs: 4
📁 Output directory: ./output_models/job_20250526_1445_qwen_Qwen2.5-32B-Instruct_FullFT

🔍 Initial GPU memory state:
GPU Memory Usage:
  GPU 0: 0.00GB allocated, 0.00GB reserved
  GPU 1: 0.00GB allocated, 0.00GB reserved
  GPU 2: 0.00GB allocated, 0.00GB reserved
  GPU 3: 0.00GB allocated, 0.00GB reserved
🔤 Loading tokenizer...
✓ Tokenizer padding side set to: left
📚 Creating datasets...
📊 Loaded 64 examples from dataset/train_set.csv
📋 Columns: ['Type', 'Case Number', 'Instruction', 'Previous Instruction', 'Input', 'Generated Output', 'Output', 'Previous Output', 'Evaluation Criteria', 'Previous Evaluation Criteria', 'Source Markdown File', 'IR Markdown File', 'IR Raw File', 'Suspect Number', 'Intelligence Number']
📊 Loaded 16 examples from dataset/test_set.csv
📋 Columns: ['Type', 'Case Number', 'Instruction', 'Previous Instruction', 'Input', 'Generated Output', 'Output', 'Previous Output', 'Evaluation Criteria', 'Previous Evaluation Criteria', 'Source Markdown File', 'IR Markdown File', 'IR Raw File', 'Suspect Number', 'Intelligence Number']

🔍 After data loading:
GPU Memory Usage:
  GPU 0: 0.00GB allocated, 0.00GB reserved
  GPU 1: 0.00GB allocated, 0.00GB reserved
  GPU 2: 0.00GB allocated, 0.00GB reserved
  GPU 3: 0.00GB allocated, 0.00GB reserved
Creating device map for 4 GPUs...
Model has 64 layers
Distributing 64 layers across 4 GPUs
Base layers per GPU: 16
GPU 0: layers 0 to 15 (16 layers)
GPU 1: layers 16 to 31 (16 layers)
GPU 2: layers 32 to 47 (16 layers)
GPU 3: layers 48 to 63 (16 layers)
🤖 Loading model with model parallelism...
✓ Model loaded with model parallelism
GPU Memory Usage:
  GPU 0: 15.98GB allocated, 15.98GB reserved
  GPU 1: 14.53GB allocated, 14.53GB reserved
  GPU 2: 14.53GB allocated, 14.53GB reserved
  GPU 3: 15.98GB allocated, 15.98GB reserved
🔧 Using full fine-tuning (no LoRA)

🔍 After model setup:
GPU Memory Usage:
  GPU 0: 15.98GB allocated, 15.98GB reserved
  GPU 1: 14.53GB allocated, 14.53GB reserved
  GPU 2: 14.53GB allocated, 14.53GB reserved
  GPU 3: 15.98GB allocated, 15.98GB reserved
🎯 All model parameters are trainable (full fine-tuning)
📊 Trainable parameters: 32,763,876,352 (100.00%)
🔧 Transformers version: 4.51.3
🔤 Tokenizer Debug Info:
  Vocab size: 151643
  Model max length: 131072
  Available special tokens:
    bos_token: None
    eos_token: '<|im_end|>' (ID: 151645) 🚨 INVALID
    pad_token: '<|endoftext|>' (ID: 151643) 🚨 INVALID
    unk_token: None
  Testing conversation tokens:
    '<|begin_of_text|>': ❌ NOT FOUND
    '<|start_header_id|>': ❌ NOT FOUND
    '<|end_header_id|>': ❌ NOT FOUND
    '<|eot_id|>': ❌ NOT FOUND
    '<|im_start|>': ID 151644 ❌ (broken/invalid)
    '<|im_end|>': ID 151645 ❌ (broken/invalid)
    '<s>': ID 128245 ✅
    '</s>': ID 128247 ✅
    '[INST]': ❌ NOT FOUND
    '[/INST]': ❌ NOT FOUND
    '<|user|>': ❌ NOT FOUND
    '<|assistant|>': ❌ NOT FOUND
    '<|system|>': ❌ NOT FOUND
    'Human:': ❌ NOT FOUND
    'Assistant:': ❌ NOT FOUND
    'User:': ❌ NOT FOUND
    'AI:': ❌ NOT FOUND
  Selected prompt format: plain_text
📊 Number of trainable parameter tensors: 771
📈 Total training steps: 48

============================================================
🚀 STARTING TRAINING
============================================================
  📊 Num examples = 64
  🔄 Num Epochs = 3
  📦 Batch size per device = 1
  📦 Total train batch size = 4
  🔄 Gradient Accumulation steps = 4
  📈 Total optimization steps = 48
============================================================

🔄 === Epoch 1/3 ===
📊 Epoch 1: Processing 64 batches...
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔍 Batch 1 debug:
  Batch keys: ['input_ids', 'attention_mask', 'labels']
  input_ids shape: torch.Size([1, 512])
  input_ids device: cpu
  Token ID range: 1 to 94506
  Loss: 0.010442
🔄 Batch 1/64: loss=0.010442, scaled=0.002610, accumulated=0.002610
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔍 Batch 2 debug:
  Batch keys: ['input_ids', 'attention_mask', 'labels']
  input_ids shape: torch.Size([1, 512])
  input_ids device: cpu
  Token ID range: 1 to 94506
  Loss: 0.000013
🔄 Batch 2/64: loss=0.000013, scaled=0.000003, accumulated=0.002614
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔍 Batch 3 debug:
  Batch keys: ['input_ids', 'attention_mask', 'labels']
  input_ids shape: torch.Size([1, 512])
  input_ids device: cpu
  Token ID range: 1 to 94506
  Loss: 0.010442
🔄 Batch 3/64: loss=0.010442, scaled=0.002610, accumulated=0.005224
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 4/64: loss=0.010442, scaled=0.002610, accumulated=0.007835
❌ Error in batch 3: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 226.88 MiB is free. Including non-PyTorch memory, this process has 78.88 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 395.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 5/64: loss=0.017710, scaled=0.004427, accumulated=0.012262
❌ Error in batch 4: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 174.88 MiB is free. Including non-PyTorch memory, this process has 78.93 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 447.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 6/64: loss=0.010442, scaled=0.002610, accumulated=0.014873
❌ Error in batch 5: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 198.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 423.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 7/64: loss=0.013093, scaled=0.003273, accumulated=0.018146
❌ Error in batch 6: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 168.88 MiB is free. Including non-PyTorch memory, this process has 78.93 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 453.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 8/64: loss=0.017710, scaled=0.004427, accumulated=0.022573
❌ Error in batch 7: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 192.88 MiB is free. Including non-PyTorch memory, this process has 78.91 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 429.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Soci...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Soc...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 9/64: loss=0.000950, scaled=0.000238, accumulated=0.022811
❌ Error in batch 8: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 162.88 MiB is free. Including non-PyTorch memory, this process has 78.94 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 459.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 10/64: loss=0.013093, scaled=0.003273, accumulated=0.026084
❌ Error in batch 9: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 190.88 MiB is free. Including non-PyTorch memory, this process has 78.91 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 431.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 11/64: loss=0.017710, scaled=0.004427, accumulated=0.030511
❌ Error in batch 10: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 158.88 MiB is free. Including non-PyTorch memory, this process has 78.94 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 463.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 12/64: loss=0.017710, scaled=0.004427, accumulated=0.034939
❌ Error in batch 11: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 188.88 MiB is free. Including non-PyTorch memory, this process has 78.91 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 433.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 13/64: loss=0.017710, scaled=0.004427, accumulated=0.039366
❌ Error in batch 12: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 156.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 465.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 14/64: loss=0.000013, scaled=0.000003, accumulated=0.039369
❌ Error in batch 13: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 186.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 435.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 15/64: loss=0.010442, scaled=0.002610, accumulated=0.041980
❌ Error in batch 14: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 16/64: loss=0.000013, scaled=0.000003, accumulated=0.041983
❌ Error in batch 15: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 17/64: loss=0.000013, scaled=0.000003, accumulated=0.041986
❌ Error in batch 16: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 18/64: loss=0.000013, scaled=0.000003, accumulated=0.041990
❌ Error in batch 17: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Soci...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Rom...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 19/64: loss=0.000950, scaled=0.000238, accumulated=0.042227
❌ Error in batch 18: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 20/64: loss=0.000013, scaled=0.000003, accumulated=0.042231
❌ Error in batch 19: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 186.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 435.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 21/64: loss=0.000013, scaled=0.000003, accumulated=0.042234
❌ Error in batch 20: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 22/64: loss=0.010442, scaled=0.002610, accumulated=0.044844
❌ Error in batch 21: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 23/64: loss=0.010442, scaled=0.002610, accumulated=0.047455
❌ Error in batch 22: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 24/64: loss=0.010442, scaled=0.002610, accumulated=0.050065
❌ Error in batch 23: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 186.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 435.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 25/64: loss=0.013093, scaled=0.003273, accumulated=0.053338
❌ Error in batch 24: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Inve...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Inv...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 26/64: loss=0.000950, scaled=0.000238, accumulated=0.053576
❌ Error in batch 25: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 27/64: loss=0.010442, scaled=0.002610, accumulated=0.056186
❌ Error in batch 26: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Inve...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Inv...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 28/64: loss=0.000950, scaled=0.000238, accumulated=0.056424
❌ Error in batch 27: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 29/64: loss=0.010442, scaled=0.002610, accumulated=0.059034
❌ Error in batch 28: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 30/64: loss=0.010442, scaled=0.002610, accumulated=0.061645
❌ Error in batch 29: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 31/64: loss=0.000013, scaled=0.000003, accumulated=0.061648
❌ Error in batch 30: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 32/64: loss=0.010442, scaled=0.002610, accumulated=0.064259
❌ Error in batch 31: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 186.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 435.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 3. HASE Suspect
| 3.HASE Suspect | Unnamed: 1 |...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 33/64: loss=0.061617, scaled=0.015404, accumulated=0.079663
❌ Error in batch 32: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 34/64: loss=0.010442, scaled=0.002610, accumulated=0.082273
❌ Error in batch 33: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 35/64: loss=0.010442, scaled=0.002610, accumulated=0.084884
❌ Error in batch 34: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 36/64: loss=0.000013, scaled=0.000003, accumulated=0.084887
❌ Error in batch 35: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 37/64: loss=0.010442, scaled=0.002610, accumulated=0.087498
❌ Error in batch 36: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 38/64: loss=0.013093, scaled=0.003273, accumulated=0.090771
❌ Error in batch 37: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 39/64: loss=0.000013, scaled=0.000003, accumulated=0.090774
❌ Error in batch 38: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Tele...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Tel...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 40/64: loss=0.000950, scaled=0.000238, accumulated=0.091012
❌ Error in batch 39: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 41/64: loss=0.010442, scaled=0.002610, accumulated=0.093622
❌ Error in batch 40: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 42/64: loss=0.017710, scaled=0.004427, accumulated=0.098049
❌ Error in batch 41: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 4.Victim with financial loss
| 4.Victim with fi...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 43/64: loss=0.061617, scaled=0.015404, accumulated=0.113454
❌ Error in batch 42: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 44/64: loss=0.017710, scaled=0.004427, accumulated=0.117881
❌ Error in batch 43: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 186.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 435.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 45/64: loss=0.013093, scaled=0.003273, accumulated=0.121154
❌ Error in batch 44: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 46/64: loss=0.017710, scaled=0.004427, accumulated=0.125582
❌ Error in batch 45: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 47/64: loss=0.013093, scaled=0.003273, accumulated=0.128855
❌ Error in batch 46: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 3. HASE Suspect
| 3.HASE Suspect | Unnamed: 1 |...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 48/64: loss=0.061617, scaled=0.015404, accumulated=0.144259
❌ Error in batch 47: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 49/64: loss=0.013093, scaled=0.003273, accumulated=0.147532
❌ Error in batch 48: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 50/64: loss=0.017710, scaled=0.004427, accumulated=0.151959
❌ Error in batch 49: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 51/64: loss=0.010442, scaled=0.002610, accumulated=0.154570
❌ Error in batch 50: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 52/64: loss=0.000013, scaled=0.000003, accumulated=0.154573
❌ Error in batch 51: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 53/64: loss=0.000013, scaled=0.000003, accumulated=0.154577
❌ Error in batch 52: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 54/64: loss=0.000013, scaled=0.000003, accumulated=0.154580
❌ Error in batch 53: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 55/64: loss=0.017710, scaled=0.004427, accumulated=0.159007
❌ Error in batch 54: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 5.Victim with financial loss
| 5.Victim with fi...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 56/64: loss=0.061617, scaled=0.015404, accumulated=0.174411
❌ Error in batch 55: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 57/64: loss=0.000013, scaled=0.000003, accumulated=0.174415
❌ Error in batch 56: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Inve...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Inv...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 58/64: loss=0.000950, scaled=0.000238, accumulated=0.174652
❌ Error in batch 57: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 59/64: loss=0.010442, scaled=0.002610, accumulated=0.177263
❌ Error in batch 58: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 60/64: loss=0.013093, scaled=0.003273, accumulated=0.180536
❌ Error in batch 59: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 61/64: loss=0.000013, scaled=0.000003, accumulated=0.180539
❌ Error in batch 60: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 4.Victim with financial loss
| 4.Victim with fi...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 62/64: loss=0.061617, scaled=0.015404, accumulated=0.195943
❌ Error in batch 61: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 63/64: loss=0.017710, scaled=0.004427, accumulated=0.200371
❌ Error in batch 62: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 64/64: loss=0.017710, scaled=0.004427, accumulated=0.204798
❌ Error in batch 63: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

✅ Epoch 1 completed:
    Average loss: 0.003200
    Duration: 64.5s
    Optimization steps this epoch: 0
    Total optimization steps so far: 0

🔄 === Epoch 2/3 ===
📊 Epoch 2: Processing 64 batches...
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔍 Batch 1 debug:
  Batch keys: ['input_ids', 'attention_mask', 'labels']
  input_ids shape: torch.Size([1, 512])
  input_ids device: cpu
  Token ID range: 1 to 94506
  Loss: 0.017710
🔄 Batch 1/64: loss=0.017710, scaled=0.004427, accumulated=0.004427
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔍 Batch 2 debug:
  Batch keys: ['input_ids', 'attention_mask', 'labels']
  input_ids shape: torch.Size([1, 512])
  input_ids device: cpu
  Token ID range: 1 to 94506
  Loss: 0.010442
🔄 Batch 2/64: loss=0.010442, scaled=0.002610, accumulated=0.007038
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔍 Batch 3 debug:
  Batch keys: ['input_ids', 'attention_mask', 'labels']
  input_ids shape: torch.Size([1, 512])
  input_ids device: cpu
  Token ID range: 1 to 94506
  Loss: 0.010442
🔄 Batch 3/64: loss=0.010442, scaled=0.002610, accumulated=0.009648
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 4/64: loss=0.000013, scaled=0.000003, accumulated=0.009652
❌ Error in batch 3: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 254.88 MiB is free. Including non-PyTorch memory, this process has 78.85 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 367.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 5/64: loss=0.010442, scaled=0.002610, accumulated=0.012262
❌ Error in batch 4: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 24.88 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 327.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 6/64: loss=0.013093, scaled=0.003273, accumulated=0.015535
❌ Error in batch 5: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 206.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 415.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Inve...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Inv...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 7/64: loss=0.000950, scaled=0.000238, accumulated=0.015773
❌ Error in batch 6: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 8/64: loss=0.010442, scaled=0.002610, accumulated=0.018383
❌ Error in batch 7: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 9/64: loss=0.010442, scaled=0.002610, accumulated=0.020994
❌ Error in batch 8: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 10/64: loss=0.000013, scaled=0.000003, accumulated=0.020997
❌ Error in batch 9: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 11/64: loss=0.010442, scaled=0.002610, accumulated=0.023607
❌ Error in batch 10: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 12/64: loss=0.013093, scaled=0.003273, accumulated=0.026881
❌ Error in batch 11: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 13/64: loss=0.017710, scaled=0.004427, accumulated=0.031308
❌ Error in batch 12: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 14/64: loss=0.000013, scaled=0.000003, accumulated=0.031311
❌ Error in batch 13: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 3. HASE Suspect
| 3.HASE Suspect | Unnamed: 1 |...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 15/64: loss=0.061617, scaled=0.015404, accumulated=0.046716
❌ Error in batch 14: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 16/64: loss=0.017710, scaled=0.004427, accumulated=0.051143
❌ Error in batch 15: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 17/64: loss=0.010442, scaled=0.002610, accumulated=0.053753
❌ Error in batch 16: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 18/64: loss=0.017710, scaled=0.004427, accumulated=0.058181
❌ Error in batch 17: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 19/64: loss=0.017710, scaled=0.004427, accumulated=0.062608
❌ Error in batch 18: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 20/64: loss=0.000013, scaled=0.000003, accumulated=0.062611
❌ Error in batch 19: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 21/64: loss=0.017710, scaled=0.004427, accumulated=0.067039
❌ Error in batch 20: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 4.Victim with financial loss
| 4.Victim with fi...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 22/64: loss=0.061617, scaled=0.015404, accumulated=0.082443
❌ Error in batch 21: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 23/64: loss=0.010442, scaled=0.002610, accumulated=0.085053
❌ Error in batch 22: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 24/64: loss=0.017710, scaled=0.004427, accumulated=0.089481
❌ Error in batch 23: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 25/64: loss=0.013093, scaled=0.003273, accumulated=0.092754
❌ Error in batch 24: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Tele...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Tel...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 26/64: loss=0.000950, scaled=0.000238, accumulated=0.092992
❌ Error in batch 25: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 27/64: loss=0.017710, scaled=0.004427, accumulated=0.097419
❌ Error in batch 26: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 28/64: loss=0.010442, scaled=0.002610, accumulated=0.100029
❌ Error in batch 27: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 29/64: loss=0.013093, scaled=0.003273, accumulated=0.103303
❌ Error in batch 28: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Soci...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Soc...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 30/64: loss=0.000950, scaled=0.000238, accumulated=0.103540
❌ Error in batch 29: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Inve...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Inv...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 31/64: loss=0.000950, scaled=0.000238, accumulated=0.103778
❌ Error in batch 30: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 32/64: loss=0.013093, scaled=0.003273, accumulated=0.107051
❌ Error in batch 31: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 33/64: loss=0.017710, scaled=0.004427, accumulated=0.111478
❌ Error in batch 32: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 34/64: loss=0.000013, scaled=0.000003, accumulated=0.111482
❌ Error in batch 33: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 35/64: loss=0.000013, scaled=0.000003, accumulated=0.111485
❌ Error in batch 34: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 36/64: loss=0.010442, scaled=0.002610, accumulated=0.114095
❌ Error in batch 35: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 37/64: loss=0.013093, scaled=0.003273, accumulated=0.117369
❌ Error in batch 36: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 38/64: loss=0.000013, scaled=0.000003, accumulated=0.117372
❌ Error in batch 37: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 39/64: loss=0.013093, scaled=0.003273, accumulated=0.120645
❌ Error in batch 38: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 40/64: loss=0.010442, scaled=0.002610, accumulated=0.123255
❌ Error in batch 39: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 41/64: loss=0.010442, scaled=0.002610, accumulated=0.125866
❌ Error in batch 40: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 42/64: loss=0.010442, scaled=0.002610, accumulated=0.128476
❌ Error in batch 41: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 43/64: loss=0.017710, scaled=0.004427, accumulated=0.132904
❌ Error in batch 42: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Soci...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Rom...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 44/64: loss=0.000950, scaled=0.000238, accumulated=0.133141
❌ Error in batch 43: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 45/64: loss=0.017710, scaled=0.004427, accumulated=0.137569
❌ Error in batch 44: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 46/64: loss=0.010442, scaled=0.002610, accumulated=0.140179
❌ Error in batch 45: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 47/64: loss=0.010442, scaled=0.002610, accumulated=0.142790
❌ Error in batch 46: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 48/64: loss=0.010442, scaled=0.002610, accumulated=0.145400
❌ Error in batch 47: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 49/64: loss=0.000013, scaled=0.000003, accumulated=0.145403
❌ Error in batch 48: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 50/64: loss=0.000013, scaled=0.000003, accumulated=0.145407
❌ Error in batch 49: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 4.Victim with financial loss
| 4.Victim with fi...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 51/64: loss=0.061617, scaled=0.015404, accumulated=0.160811
❌ Error in batch 50: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 52/64: loss=0.017710, scaled=0.004427, accumulated=0.165238
❌ Error in batch 51: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 53/64: loss=0.000013, scaled=0.000003, accumulated=0.165242
❌ Error in batch 52: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 54/64: loss=0.000013, scaled=0.000003, accumulated=0.165245
❌ Error in batch 53: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 55/64: loss=0.013093, scaled=0.003273, accumulated=0.168518
❌ Error in batch 54: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Inve...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Inv...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 56/64: loss=0.000950, scaled=0.000238, accumulated=0.168756
❌ Error in batch 55: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 57/64: loss=0.000013, scaled=0.000003, accumulated=0.168759
❌ Error in batch 56: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 58/64: loss=0.000013, scaled=0.000003, accumulated=0.168762
❌ Error in batch 57: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 59/64: loss=0.000013, scaled=0.000003, accumulated=0.168766
❌ Error in batch 58: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 60/64: loss=0.010442, scaled=0.002610, accumulated=0.171376
❌ Error in batch 59: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 61/64: loss=0.010442, scaled=0.002610, accumulated=0.173986
❌ Error in batch 60: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 62/64: loss=0.000013, scaled=0.000003, accumulated=0.173990
❌ Error in batch 61: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 3. HASE Suspect
| 3.HASE Suspect | Unnamed: 1 |...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 63/64: loss=0.061617, scaled=0.015404, accumulated=0.189394
❌ Error in batch 62: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 5.Victim with financial loss
| 5.Victim with fi...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 64/64: loss=0.061617, scaled=0.015404, accumulated=0.204798
❌ Error in batch 63: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

✅ Epoch 2 completed:
    Average loss: 0.003200
    Duration: 58.7s
    Optimization steps this epoch: -16
    Total optimization steps so far: 0

🔄 === Epoch 3/3 ===
📊 Epoch 3: Processing 64 batches...
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔍 Batch 1 debug:
  Batch keys: ['input_ids', 'attention_mask', 'labels']
  input_ids shape: torch.Size([1, 512])
  input_ids device: cpu
  Token ID range: 1 to 94506
  Loss: 0.000013
🔄 Batch 1/64: loss=0.000013, scaled=0.000003, accumulated=0.000003
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔍 Batch 2 debug:
  Batch keys: ['input_ids', 'attention_mask', 'labels']
  input_ids shape: torch.Size([1, 512])
  input_ids device: cpu
  Token ID range: 1 to 94506
  Loss: 0.017710
🔄 Batch 2/64: loss=0.017710, scaled=0.004427, accumulated=0.004431
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Inve...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Inv...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔍 Batch 3 debug:
  Batch keys: ['input_ids', 'attention_mask', 'labels']
  input_ids shape: torch.Size([1, 512])
  input_ids device: cpu
  Token ID range: 1 to 94506
  Loss: 0.000950
🔄 Batch 3/64: loss=0.000950, scaled=0.000238, accumulated=0.004668
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 4/64: loss=0.000013, scaled=0.000003, accumulated=0.004672
❌ Error in batch 3: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 254.88 MiB is free. Including non-PyTorch memory, this process has 78.85 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 367.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 5/64: loss=0.010442, scaled=0.002610, accumulated=0.007282
❌ Error in batch 4: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 24.88 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 327.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 6/64: loss=0.000013, scaled=0.000003, accumulated=0.007285
❌ Error in batch 5: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 206.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 415.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 4.Victim with financial loss
| 4.Victim with fi...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 7/64: loss=0.061617, scaled=0.015404, accumulated=0.022690
❌ Error in batch 6: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 8/64: loss=0.010442, scaled=0.002610, accumulated=0.025300
❌ Error in batch 7: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 9/64: loss=0.013093, scaled=0.003273, accumulated=0.028573
❌ Error in batch 8: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 10/64: loss=0.013093, scaled=0.003273, accumulated=0.031846
❌ Error in batch 9: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 11/64: loss=0.017710, scaled=0.004427, accumulated=0.036274
❌ Error in batch 10: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 12/64: loss=0.000013, scaled=0.000003, accumulated=0.036277
❌ Error in batch 11: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 13/64: loss=0.010442, scaled=0.002610, accumulated=0.038887
❌ Error in batch 12: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 14/64: loss=0.010442, scaled=0.002610, accumulated=0.041498
❌ Error in batch 13: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 15/64: loss=0.010442, scaled=0.002610, accumulated=0.044108
❌ Error in batch 14: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 16/64: loss=0.017710, scaled=0.004427, accumulated=0.048536
❌ Error in batch 15: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 17/64: loss=0.017710, scaled=0.004427, accumulated=0.052963
❌ Error in batch 16: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Tele...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Tel...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 18/64: loss=0.000950, scaled=0.000238, accumulated=0.053201
❌ Error in batch 17: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 19/64: loss=0.010442, scaled=0.002610, accumulated=0.055811
❌ Error in batch 18: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 20/64: loss=0.010442, scaled=0.002610, accumulated=0.058422
❌ Error in batch 19: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 21/64: loss=0.017710, scaled=0.004427, accumulated=0.062849
❌ Error in batch 20: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 3. HASE Suspect
| 3.HASE Suspect | Unnamed: 1 |...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 22/64: loss=0.061617, scaled=0.015404, accumulated=0.078253
❌ Error in batch 21: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 23/64: loss=0.000013, scaled=0.000003, accumulated=0.078257
❌ Error in batch 22: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 24/64: loss=0.013093, scaled=0.003273, accumulated=0.081530
❌ Error in batch 23: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 25/64: loss=0.010442, scaled=0.002610, accumulated=0.084140
❌ Error in batch 24: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 26/64: loss=0.000013, scaled=0.000003, accumulated=0.084143
❌ Error in batch 25: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 27/64: loss=0.000013, scaled=0.000003, accumulated=0.084147
❌ Error in batch 26: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 28/64: loss=0.017710, scaled=0.004427, accumulated=0.088574
❌ Error in batch 27: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 29/64: loss=0.010442, scaled=0.002610, accumulated=0.091185
❌ Error in batch 28: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 30/64: loss=0.013093, scaled=0.003273, accumulated=0.094458
❌ Error in batch 29: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 31/64: loss=0.013093, scaled=0.003273, accumulated=0.097731
❌ Error in batch 30: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Inve...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Inv...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 32/64: loss=0.000950, scaled=0.000238, accumulated=0.097968
❌ Error in batch 31: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 33/64: loss=0.017710, scaled=0.004427, accumulated=0.102396
❌ Error in batch 32: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 34/64: loss=0.017710, scaled=0.004427, accumulated=0.106823
❌ Error in batch 33: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 35/64: loss=0.010442, scaled=0.002610, accumulated=0.109434
❌ Error in batch 34: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 36/64: loss=0.017710, scaled=0.004427, accumulated=0.113861
❌ Error in batch 35: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 37/64: loss=0.010442, scaled=0.002610, accumulated=0.116472
❌ Error in batch 36: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 38/64: loss=0.010442, scaled=0.002610, accumulated=0.119082
❌ Error in batch 37: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 4.Victim with financial loss
| 4.Victim with fi...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 39/64: loss=0.061617, scaled=0.015404, accumulated=0.134486
❌ Error in batch 38: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Soci...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Rom...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 40/64: loss=0.000950, scaled=0.000238, accumulated=0.134724
❌ Error in batch 39: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 41/64: loss=0.010442, scaled=0.002610, accumulated=0.137334
❌ Error in batch 40: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 42/64: loss=0.013093, scaled=0.003273, accumulated=0.140607
❌ Error in batch 41: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 43/64: loss=0.010442, scaled=0.002610, accumulated=0.143218
❌ Error in batch 42: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 44/64: loss=0.010442, scaled=0.002610, accumulated=0.145828
❌ Error in batch 43: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 45/64: loss=0.000013, scaled=0.000003, accumulated=0.145832
❌ Error in batch 44: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 46/64: loss=0.017710, scaled=0.004427, accumulated=0.150259
❌ Error in batch 45: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 47/64: loss=0.017710, scaled=0.004427, accumulated=0.154686
❌ Error in batch 46: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Soci...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Soc...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 48/64: loss=0.000950, scaled=0.000238, accumulated=0.154924
❌ Error in batch 47: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 49/64: loss=0.000013, scaled=0.000003, accumulated=0.154927
❌ Error in batch 48: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 50/64: loss=0.010442, scaled=0.002610, accumulated=0.157538
❌ Error in batch 49: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 51/64: loss=0.000013, scaled=0.000003, accumulated=0.157541
❌ Error in batch 50: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 52/64: loss=0.000013, scaled=0.000003, accumulated=0.157544
❌ Error in batch 51: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 5.Victim with financial loss
| 5.Victim with fi...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 53/64: loss=0.061617, scaled=0.015404, accumulated=0.172949
❌ Error in batch 52: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 54/64: loss=0.000013, scaled=0.000003, accumulated=0.172952
❌ Error in batch 53: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 55/64: loss=0.013093, scaled=0.003273, accumulated=0.176225
❌ Error in batch 54: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 56/64: loss=0.010442, scaled=0.002610, accumulated=0.178835
❌ Error in batch 55: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 3. HASE Suspect
| 3.HASE Suspect | Unnamed: 1 |...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 57/64: loss=0.061617, scaled=0.015404, accumulated=0.194240
❌ Error in batch 56: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 58/64: loss=0.000013, scaled=0.000003, accumulated=0.194243
❌ Error in batch 57: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 59/64: loss=0.017710, scaled=0.004427, accumulated=0.198670
❌ Error in batch 58: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 60/64: loss=0.013093, scaled=0.003273, accumulated=0.201943
❌ Error in batch 59: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 61/64: loss=0.000013, scaled=0.000003, accumulated=0.201947
❌ Error in batch 60: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 62/64: loss=0.000013, scaled=0.000003, accumulated=0.201950
❌ Error in batch 61: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Inve...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Inv...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 63/64: loss=0.000950, scaled=0.000238, accumulated=0.202188
❌ Error in batch 62: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
🔍 Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
✅ Successfully processed 1 items in batch
📊 Final batch shape: torch.Size([1, 512])
📊 Token ID range: 1 to 94506
🔄 Batch 64/64: loss=0.010442, scaled=0.002610, accumulated=0.204798
❌ Error in batch 63: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

✅ Epoch 3 completed:
    Average loss: 0.003200
    Duration: 58.8s
    Optimization steps this epoch: -32
    Total optimization steps so far: 0

🎉 Training completed! Total optimization steps: 0
🚨 WARNING: No optimization steps were performed!
🚨 The model was not actually trained!

==================================================
❌ MODEL PARALLEL TRAINING FAILED!
==================================================

==================================================
STARTING CLEANUP PROCESS
==================================================
Closing TensorBoard writer...
Clearing model...
Cleanup completed!
==================================================
🧹 All cleanup completed. Exiting.

Program exiting normally...

==================================================
STARTING CLEANUP PROCESS
==================================================
Cleanup completed!
==================================================
Found 0 child processes to terminate
