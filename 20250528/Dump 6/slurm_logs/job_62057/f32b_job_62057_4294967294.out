
=============
== PyTorch ==
=============

NVIDIA Release 24.08 (build 107063150)
PyTorch Version 2.5.0a0+872d972
Container image Copyright (c) 2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
Copyright (c) 2014-2024 Facebook Inc.
Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)
Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)
Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)
Copyright (c) 2011-2013 NYU                      (Clement Farabet)
Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)
Copyright (c) 2006      Idiap Research Institute (Samy Bengio)
Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)
Copyright (c) 2015      Google Inc.
Copyright (c) 2015      Yangqing Jia
Copyright (c) 2013-2016 The Caffe contributors
All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

ERROR: This container was built for NVIDIA Driver Release 560.35 or later, but
       version 535.161.08 was detected and compatibility mode is UNAVAILABLE.

       [[]]

Looking in links: /cm/shared/python_modules/
Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.51.3)
Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.7.0)
Requirement already satisfied: deepspeed in /usr/local/lib/python3.10/dist-packages (0.16.0)
Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)
Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.31.4)
Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.4)
Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)
Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.7.24)
Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)
Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.1)
Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.3)
Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)
Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.0)
Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.0a0+872d972e41.nv24.8)
Requirement already satisfied: hjson in /usr/local/lib/python3.10/dist-packages (from deepspeed) (3.1.0)
Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.0.8)
Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.11.1.1)
Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)
Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.8.2)
Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.10/dist-packages (from deepspeed) (12.560.30)
Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)
Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)
Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (2.20.1)
Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.3)
Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.4)
Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)
Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)
[2025-05-26 14:45:09,971] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/users/1787/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Configuration loaded from config.json
âœ“ Configuration validation passed
============================================================
LOADED CONFIGURATION
============================================================
ğŸ“ Model Configuration:
  Model path: /cm/shared/llm_models/Qwen/Qwen2.5-32B-Instruct
  Model family: qwen

ğŸ“Š Data Configuration:
  Training data: dataset/train_set.csv
  Evaluation data: dataset/test_set.csv

ğŸ”§ LoRA Configuration:
  Use LoRA: False

ğŸš€ Training Configuration:
  Batch size: 1
  Learning rate: 3e-05
  Number of epochs: 3
  Max sequence length: 512
  Gradient accumulation steps: 4
  Warmup steps: 10

ğŸ’¾ Output Configuration:
  Output directory: Auto-generated
============================================================

ğŸ–¥ï¸  Available GPUs: 4
ğŸ“ Output directory: ./output_models/job_20250526_1445_qwen_Qwen2.5-32B-Instruct_FullFT

ğŸ” Initial GPU memory state:
GPU Memory Usage:
  GPU 0: 0.00GB allocated, 0.00GB reserved
  GPU 1: 0.00GB allocated, 0.00GB reserved
  GPU 2: 0.00GB allocated, 0.00GB reserved
  GPU 3: 0.00GB allocated, 0.00GB reserved
ğŸ”¤ Loading tokenizer...
âœ“ Tokenizer padding side set to: left
ğŸ“š Creating datasets...
ğŸ“Š Loaded 64 examples from dataset/train_set.csv
ğŸ“‹ Columns: ['Type', 'Case Number', 'Instruction', 'Previous Instruction', 'Input', 'Generated Output', 'Output', 'Previous Output', 'Evaluation Criteria', 'Previous Evaluation Criteria', 'Source Markdown File', 'IR Markdown File', 'IR Raw File', 'Suspect Number', 'Intelligence Number']
ğŸ“Š Loaded 16 examples from dataset/test_set.csv
ğŸ“‹ Columns: ['Type', 'Case Number', 'Instruction', 'Previous Instruction', 'Input', 'Generated Output', 'Output', 'Previous Output', 'Evaluation Criteria', 'Previous Evaluation Criteria', 'Source Markdown File', 'IR Markdown File', 'IR Raw File', 'Suspect Number', 'Intelligence Number']

ğŸ” After data loading:
GPU Memory Usage:
  GPU 0: 0.00GB allocated, 0.00GB reserved
  GPU 1: 0.00GB allocated, 0.00GB reserved
  GPU 2: 0.00GB allocated, 0.00GB reserved
  GPU 3: 0.00GB allocated, 0.00GB reserved
Creating device map for 4 GPUs...
Model has 64 layers
Distributing 64 layers across 4 GPUs
Base layers per GPU: 16
GPU 0: layers 0 to 15 (16 layers)
GPU 1: layers 16 to 31 (16 layers)
GPU 2: layers 32 to 47 (16 layers)
GPU 3: layers 48 to 63 (16 layers)
ğŸ¤– Loading model with model parallelism...
âœ“ Model loaded with model parallelism
GPU Memory Usage:
  GPU 0: 15.98GB allocated, 15.98GB reserved
  GPU 1: 14.53GB allocated, 14.53GB reserved
  GPU 2: 14.53GB allocated, 14.53GB reserved
  GPU 3: 15.98GB allocated, 15.98GB reserved
ğŸ”§ Using full fine-tuning (no LoRA)

ğŸ” After model setup:
GPU Memory Usage:
  GPU 0: 15.98GB allocated, 15.98GB reserved
  GPU 1: 14.53GB allocated, 14.53GB reserved
  GPU 2: 14.53GB allocated, 14.53GB reserved
  GPU 3: 15.98GB allocated, 15.98GB reserved
ğŸ¯ All model parameters are trainable (full fine-tuning)
ğŸ“Š Trainable parameters: 32,763,876,352 (100.00%)
ğŸ”§ Transformers version: 4.51.3
ğŸ”¤ Tokenizer Debug Info:
  Vocab size: 151643
  Model max length: 131072
  Available special tokens:
    bos_token: None
    eos_token: '<|im_end|>' (ID: 151645) ğŸš¨ INVALID
    pad_token: '<|endoftext|>' (ID: 151643) ğŸš¨ INVALID
    unk_token: None
  Testing conversation tokens:
    '<|begin_of_text|>': âŒ NOT FOUND
    '<|start_header_id|>': âŒ NOT FOUND
    '<|end_header_id|>': âŒ NOT FOUND
    '<|eot_id|>': âŒ NOT FOUND
    '<|im_start|>': ID 151644 âŒ (broken/invalid)
    '<|im_end|>': ID 151645 âŒ (broken/invalid)
    '<s>': ID 128245 âœ…
    '</s>': ID 128247 âœ…
    '[INST]': âŒ NOT FOUND
    '[/INST]': âŒ NOT FOUND
    '<|user|>': âŒ NOT FOUND
    '<|assistant|>': âŒ NOT FOUND
    '<|system|>': âŒ NOT FOUND
    'Human:': âŒ NOT FOUND
    'Assistant:': âŒ NOT FOUND
    'User:': âŒ NOT FOUND
    'AI:': âŒ NOT FOUND
  Selected prompt format: plain_text
ğŸ“Š Number of trainable parameter tensors: 771
ğŸ“ˆ Total training steps: 48

============================================================
ğŸš€ STARTING TRAINING
============================================================
  ğŸ“Š Num examples = 64
  ğŸ”„ Num Epochs = 3
  ğŸ“¦ Batch size per device = 1
  ğŸ“¦ Total train batch size = 4
  ğŸ”„ Gradient Accumulation steps = 4
  ğŸ“ˆ Total optimization steps = 48
============================================================

ğŸ”„ === Epoch 1/3 ===
ğŸ“Š Epoch 1: Processing 64 batches...
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ” Batch 1 debug:
  Batch keys: ['input_ids', 'attention_mask', 'labels']
  input_ids shape: torch.Size([1, 512])
  input_ids device: cpu
  Token ID range: 1 to 94506
  Loss: 0.010442
ğŸ”„ Batch 1/64: loss=0.010442, scaled=0.002610, accumulated=0.002610
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ” Batch 2 debug:
  Batch keys: ['input_ids', 'attention_mask', 'labels']
  input_ids shape: torch.Size([1, 512])
  input_ids device: cpu
  Token ID range: 1 to 94506
  Loss: 0.000013
ğŸ”„ Batch 2/64: loss=0.000013, scaled=0.000003, accumulated=0.002614
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ” Batch 3 debug:
  Batch keys: ['input_ids', 'attention_mask', 'labels']
  input_ids shape: torch.Size([1, 512])
  input_ids device: cpu
  Token ID range: 1 to 94506
  Loss: 0.010442
ğŸ”„ Batch 3/64: loss=0.010442, scaled=0.002610, accumulated=0.005224
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 4/64: loss=0.010442, scaled=0.002610, accumulated=0.007835
âŒ Error in batch 3: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 226.88 MiB is free. Including non-PyTorch memory, this process has 78.88 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 395.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 5/64: loss=0.017710, scaled=0.004427, accumulated=0.012262
âŒ Error in batch 4: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 174.88 MiB is free. Including non-PyTorch memory, this process has 78.93 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 447.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 6/64: loss=0.010442, scaled=0.002610, accumulated=0.014873
âŒ Error in batch 5: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 198.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 423.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 7/64: loss=0.013093, scaled=0.003273, accumulated=0.018146
âŒ Error in batch 6: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 168.88 MiB is free. Including non-PyTorch memory, this process has 78.93 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 453.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 8/64: loss=0.017710, scaled=0.004427, accumulated=0.022573
âŒ Error in batch 7: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 192.88 MiB is free. Including non-PyTorch memory, this process has 78.91 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 429.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Soci...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Soc...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 9/64: loss=0.000950, scaled=0.000238, accumulated=0.022811
âŒ Error in batch 8: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 162.88 MiB is free. Including non-PyTorch memory, this process has 78.94 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 459.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 10/64: loss=0.013093, scaled=0.003273, accumulated=0.026084
âŒ Error in batch 9: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 190.88 MiB is free. Including non-PyTorch memory, this process has 78.91 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 431.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 11/64: loss=0.017710, scaled=0.004427, accumulated=0.030511
âŒ Error in batch 10: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 158.88 MiB is free. Including non-PyTorch memory, this process has 78.94 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 463.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 12/64: loss=0.017710, scaled=0.004427, accumulated=0.034939
âŒ Error in batch 11: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 188.88 MiB is free. Including non-PyTorch memory, this process has 78.91 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 433.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 13/64: loss=0.017710, scaled=0.004427, accumulated=0.039366
âŒ Error in batch 12: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 156.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 465.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 14/64: loss=0.000013, scaled=0.000003, accumulated=0.039369
âŒ Error in batch 13: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 186.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 435.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 15/64: loss=0.010442, scaled=0.002610, accumulated=0.041980
âŒ Error in batch 14: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 16/64: loss=0.000013, scaled=0.000003, accumulated=0.041983
âŒ Error in batch 15: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 17/64: loss=0.000013, scaled=0.000003, accumulated=0.041986
âŒ Error in batch 16: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 18/64: loss=0.000013, scaled=0.000003, accumulated=0.041990
âŒ Error in batch 17: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Soci...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Rom...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 19/64: loss=0.000950, scaled=0.000238, accumulated=0.042227
âŒ Error in batch 18: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 20/64: loss=0.000013, scaled=0.000003, accumulated=0.042231
âŒ Error in batch 19: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 186.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 435.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 21/64: loss=0.000013, scaled=0.000003, accumulated=0.042234
âŒ Error in batch 20: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 22/64: loss=0.010442, scaled=0.002610, accumulated=0.044844
âŒ Error in batch 21: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 23/64: loss=0.010442, scaled=0.002610, accumulated=0.047455
âŒ Error in batch 22: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 24/64: loss=0.010442, scaled=0.002610, accumulated=0.050065
âŒ Error in batch 23: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 186.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 435.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 25/64: loss=0.013093, scaled=0.003273, accumulated=0.053338
âŒ Error in batch 24: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Inve...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Inv...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 26/64: loss=0.000950, scaled=0.000238, accumulated=0.053576
âŒ Error in batch 25: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 27/64: loss=0.010442, scaled=0.002610, accumulated=0.056186
âŒ Error in batch 26: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Inve...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Inv...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 28/64: loss=0.000950, scaled=0.000238, accumulated=0.056424
âŒ Error in batch 27: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 29/64: loss=0.010442, scaled=0.002610, accumulated=0.059034
âŒ Error in batch 28: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 30/64: loss=0.010442, scaled=0.002610, accumulated=0.061645
âŒ Error in batch 29: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 31/64: loss=0.000013, scaled=0.000003, accumulated=0.061648
âŒ Error in batch 30: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 32/64: loss=0.010442, scaled=0.002610, accumulated=0.064259
âŒ Error in batch 31: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 186.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 435.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 3. HASE Suspect
| 3.HASE Suspect | Unnamed: 1 |...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 33/64: loss=0.061617, scaled=0.015404, accumulated=0.079663
âŒ Error in batch 32: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 34/64: loss=0.010442, scaled=0.002610, accumulated=0.082273
âŒ Error in batch 33: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 35/64: loss=0.010442, scaled=0.002610, accumulated=0.084884
âŒ Error in batch 34: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 36/64: loss=0.000013, scaled=0.000003, accumulated=0.084887
âŒ Error in batch 35: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 37/64: loss=0.010442, scaled=0.002610, accumulated=0.087498
âŒ Error in batch 36: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 38/64: loss=0.013093, scaled=0.003273, accumulated=0.090771
âŒ Error in batch 37: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 39/64: loss=0.000013, scaled=0.000003, accumulated=0.090774
âŒ Error in batch 38: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Tele...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Tel...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 40/64: loss=0.000950, scaled=0.000238, accumulated=0.091012
âŒ Error in batch 39: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 41/64: loss=0.010442, scaled=0.002610, accumulated=0.093622
âŒ Error in batch 40: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 42/64: loss=0.017710, scaled=0.004427, accumulated=0.098049
âŒ Error in batch 41: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 4.Victim with financial loss
| 4.Victim with fi...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 43/64: loss=0.061617, scaled=0.015404, accumulated=0.113454
âŒ Error in batch 42: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 44/64: loss=0.017710, scaled=0.004427, accumulated=0.117881
âŒ Error in batch 43: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 186.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 435.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 45/64: loss=0.013093, scaled=0.003273, accumulated=0.121154
âŒ Error in batch 44: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 46/64: loss=0.017710, scaled=0.004427, accumulated=0.125582
âŒ Error in batch 45: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 47/64: loss=0.013093, scaled=0.003273, accumulated=0.128855
âŒ Error in batch 46: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 3. HASE Suspect
| 3.HASE Suspect | Unnamed: 1 |...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 48/64: loss=0.061617, scaled=0.015404, accumulated=0.144259
âŒ Error in batch 47: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 49/64: loss=0.013093, scaled=0.003273, accumulated=0.147532
âŒ Error in batch 48: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 50/64: loss=0.017710, scaled=0.004427, accumulated=0.151959
âŒ Error in batch 49: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 51/64: loss=0.010442, scaled=0.002610, accumulated=0.154570
âŒ Error in batch 50: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 52/64: loss=0.000013, scaled=0.000003, accumulated=0.154573
âŒ Error in batch 51: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 53/64: loss=0.000013, scaled=0.000003, accumulated=0.154577
âŒ Error in batch 52: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 54/64: loss=0.000013, scaled=0.000003, accumulated=0.154580
âŒ Error in batch 53: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 55/64: loss=0.017710, scaled=0.004427, accumulated=0.159007
âŒ Error in batch 54: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 5.Victim with financial loss
| 5.Victim with fi...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 56/64: loss=0.061617, scaled=0.015404, accumulated=0.174411
âŒ Error in batch 55: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 57/64: loss=0.000013, scaled=0.000003, accumulated=0.174415
âŒ Error in batch 56: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Inve...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Inv...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 58/64: loss=0.000950, scaled=0.000238, accumulated=0.174652
âŒ Error in batch 57: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 59/64: loss=0.010442, scaled=0.002610, accumulated=0.177263
âŒ Error in batch 58: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 60/64: loss=0.013093, scaled=0.003273, accumulated=0.180536
âŒ Error in batch 59: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 61/64: loss=0.000013, scaled=0.000003, accumulated=0.180539
âŒ Error in batch 60: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 4.Victim with financial loss
| 4.Victim with fi...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 62/64: loss=0.061617, scaled=0.015404, accumulated=0.195943
âŒ Error in batch 61: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 63/64: loss=0.017710, scaled=0.004427, accumulated=0.200371
âŒ Error in batch 62: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 154.88 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 467.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 64/64: loss=0.017710, scaled=0.004427, accumulated=0.204798
âŒ Error in batch 63: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 184.88 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 437.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

âœ… Epoch 1 completed:
    Average loss: 0.003200
    Duration: 64.5s
    Optimization steps this epoch: 0
    Total optimization steps so far: 0

ğŸ”„ === Epoch 2/3 ===
ğŸ“Š Epoch 2: Processing 64 batches...
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ” Batch 1 debug:
  Batch keys: ['input_ids', 'attention_mask', 'labels']
  input_ids shape: torch.Size([1, 512])
  input_ids device: cpu
  Token ID range: 1 to 94506
  Loss: 0.017710
ğŸ”„ Batch 1/64: loss=0.017710, scaled=0.004427, accumulated=0.004427
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ” Batch 2 debug:
  Batch keys: ['input_ids', 'attention_mask', 'labels']
  input_ids shape: torch.Size([1, 512])
  input_ids device: cpu
  Token ID range: 1 to 94506
  Loss: 0.010442
ğŸ”„ Batch 2/64: loss=0.010442, scaled=0.002610, accumulated=0.007038
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ” Batch 3 debug:
  Batch keys: ['input_ids', 'attention_mask', 'labels']
  input_ids shape: torch.Size([1, 512])
  input_ids device: cpu
  Token ID range: 1 to 94506
  Loss: 0.010442
ğŸ”„ Batch 3/64: loss=0.010442, scaled=0.002610, accumulated=0.009648
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 4/64: loss=0.000013, scaled=0.000003, accumulated=0.009652
âŒ Error in batch 3: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 254.88 MiB is free. Including non-PyTorch memory, this process has 78.85 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 367.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 5/64: loss=0.010442, scaled=0.002610, accumulated=0.012262
âŒ Error in batch 4: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 24.88 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 327.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 6/64: loss=0.013093, scaled=0.003273, accumulated=0.015535
âŒ Error in batch 5: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 206.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 415.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Inve...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Inv...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 7/64: loss=0.000950, scaled=0.000238, accumulated=0.015773
âŒ Error in batch 6: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 8/64: loss=0.010442, scaled=0.002610, accumulated=0.018383
âŒ Error in batch 7: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 9/64: loss=0.010442, scaled=0.002610, accumulated=0.020994
âŒ Error in batch 8: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 10/64: loss=0.000013, scaled=0.000003, accumulated=0.020997
âŒ Error in batch 9: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 11/64: loss=0.010442, scaled=0.002610, accumulated=0.023607
âŒ Error in batch 10: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 12/64: loss=0.013093, scaled=0.003273, accumulated=0.026881
âŒ Error in batch 11: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 13/64: loss=0.017710, scaled=0.004427, accumulated=0.031308
âŒ Error in batch 12: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 14/64: loss=0.000013, scaled=0.000003, accumulated=0.031311
âŒ Error in batch 13: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 3. HASE Suspect
| 3.HASE Suspect | Unnamed: 1 |...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 15/64: loss=0.061617, scaled=0.015404, accumulated=0.046716
âŒ Error in batch 14: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 16/64: loss=0.017710, scaled=0.004427, accumulated=0.051143
âŒ Error in batch 15: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 17/64: loss=0.010442, scaled=0.002610, accumulated=0.053753
âŒ Error in batch 16: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 18/64: loss=0.017710, scaled=0.004427, accumulated=0.058181
âŒ Error in batch 17: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 19/64: loss=0.017710, scaled=0.004427, accumulated=0.062608
âŒ Error in batch 18: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 20/64: loss=0.000013, scaled=0.000003, accumulated=0.062611
âŒ Error in batch 19: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 21/64: loss=0.017710, scaled=0.004427, accumulated=0.067039
âŒ Error in batch 20: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 4.Victim with financial loss
| 4.Victim with fi...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 22/64: loss=0.061617, scaled=0.015404, accumulated=0.082443
âŒ Error in batch 21: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 23/64: loss=0.010442, scaled=0.002610, accumulated=0.085053
âŒ Error in batch 22: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 24/64: loss=0.017710, scaled=0.004427, accumulated=0.089481
âŒ Error in batch 23: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 25/64: loss=0.013093, scaled=0.003273, accumulated=0.092754
âŒ Error in batch 24: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Tele...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Tel...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 26/64: loss=0.000950, scaled=0.000238, accumulated=0.092992
âŒ Error in batch 25: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 27/64: loss=0.017710, scaled=0.004427, accumulated=0.097419
âŒ Error in batch 26: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 28/64: loss=0.010442, scaled=0.002610, accumulated=0.100029
âŒ Error in batch 27: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 29/64: loss=0.013093, scaled=0.003273, accumulated=0.103303
âŒ Error in batch 28: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Soci...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Soc...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 30/64: loss=0.000950, scaled=0.000238, accumulated=0.103540
âŒ Error in batch 29: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Inve...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Inv...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 31/64: loss=0.000950, scaled=0.000238, accumulated=0.103778
âŒ Error in batch 30: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 32/64: loss=0.013093, scaled=0.003273, accumulated=0.107051
âŒ Error in batch 31: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 33/64: loss=0.017710, scaled=0.004427, accumulated=0.111478
âŒ Error in batch 32: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 34/64: loss=0.000013, scaled=0.000003, accumulated=0.111482
âŒ Error in batch 33: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 35/64: loss=0.000013, scaled=0.000003, accumulated=0.111485
âŒ Error in batch 34: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 36/64: loss=0.010442, scaled=0.002610, accumulated=0.114095
âŒ Error in batch 35: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 37/64: loss=0.013093, scaled=0.003273, accumulated=0.117369
âŒ Error in batch 36: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 38/64: loss=0.000013, scaled=0.000003, accumulated=0.117372
âŒ Error in batch 37: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 39/64: loss=0.013093, scaled=0.003273, accumulated=0.120645
âŒ Error in batch 38: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 40/64: loss=0.010442, scaled=0.002610, accumulated=0.123255
âŒ Error in batch 39: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 41/64: loss=0.010442, scaled=0.002610, accumulated=0.125866
âŒ Error in batch 40: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 42/64: loss=0.010442, scaled=0.002610, accumulated=0.128476
âŒ Error in batch 41: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 43/64: loss=0.017710, scaled=0.004427, accumulated=0.132904
âŒ Error in batch 42: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Soci...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Rom...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 44/64: loss=0.000950, scaled=0.000238, accumulated=0.133141
âŒ Error in batch 43: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 45/64: loss=0.017710, scaled=0.004427, accumulated=0.137569
âŒ Error in batch 44: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 46/64: loss=0.010442, scaled=0.002610, accumulated=0.140179
âŒ Error in batch 45: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 47/64: loss=0.010442, scaled=0.002610, accumulated=0.142790
âŒ Error in batch 46: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 48/64: loss=0.010442, scaled=0.002610, accumulated=0.145400
âŒ Error in batch 47: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 49/64: loss=0.000013, scaled=0.000003, accumulated=0.145403
âŒ Error in batch 48: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 50/64: loss=0.000013, scaled=0.000003, accumulated=0.145407
âŒ Error in batch 49: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 4.Victim with financial loss
| 4.Victim with fi...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 51/64: loss=0.061617, scaled=0.015404, accumulated=0.160811
âŒ Error in batch 50: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 52/64: loss=0.017710, scaled=0.004427, accumulated=0.165238
âŒ Error in batch 51: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 53/64: loss=0.000013, scaled=0.000003, accumulated=0.165242
âŒ Error in batch 52: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 54/64: loss=0.000013, scaled=0.000003, accumulated=0.165245
âŒ Error in batch 53: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 55/64: loss=0.013093, scaled=0.003273, accumulated=0.168518
âŒ Error in batch 54: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Inve...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Inv...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 56/64: loss=0.000950, scaled=0.000238, accumulated=0.168756
âŒ Error in batch 55: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 57/64: loss=0.000013, scaled=0.000003, accumulated=0.168759
âŒ Error in batch 56: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 58/64: loss=0.000013, scaled=0.000003, accumulated=0.168762
âŒ Error in batch 57: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 59/64: loss=0.000013, scaled=0.000003, accumulated=0.168766
âŒ Error in batch 58: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 60/64: loss=0.010442, scaled=0.002610, accumulated=0.171376
âŒ Error in batch 59: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 61/64: loss=0.010442, scaled=0.002610, accumulated=0.173986
âŒ Error in batch 60: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 62/64: loss=0.000013, scaled=0.000003, accumulated=0.173990
âŒ Error in batch 61: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 3. HASE Suspect
| 3.HASE Suspect | Unnamed: 1 |...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 63/64: loss=0.061617, scaled=0.015404, accumulated=0.189394
âŒ Error in batch 62: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 5.Victim with financial loss
| 5.Victim with fi...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 64/64: loss=0.061617, scaled=0.015404, accumulated=0.204798
âŒ Error in batch 63: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

âœ… Epoch 2 completed:
    Average loss: 0.003200
    Duration: 58.7s
    Optimization steps this epoch: -16
    Total optimization steps so far: 0

ğŸ”„ === Epoch 3/3 ===
ğŸ“Š Epoch 3: Processing 64 batches...
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ” Batch 1 debug:
  Batch keys: ['input_ids', 'attention_mask', 'labels']
  input_ids shape: torch.Size([1, 512])
  input_ids device: cpu
  Token ID range: 1 to 94506
  Loss: 0.000013
ğŸ”„ Batch 1/64: loss=0.000013, scaled=0.000003, accumulated=0.000003
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ” Batch 2 debug:
  Batch keys: ['input_ids', 'attention_mask', 'labels']
  input_ids shape: torch.Size([1, 512])
  input_ids device: cpu
  Token ID range: 1 to 94506
  Loss: 0.017710
ğŸ”„ Batch 2/64: loss=0.017710, scaled=0.004427, accumulated=0.004431
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Inve...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Inv...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ” Batch 3 debug:
  Batch keys: ['input_ids', 'attention_mask', 'labels']
  input_ids shape: torch.Size([1, 512])
  input_ids device: cpu
  Token ID range: 1 to 94506
  Loss: 0.000950
ğŸ”„ Batch 3/64: loss=0.000950, scaled=0.000238, accumulated=0.004668
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 4/64: loss=0.000013, scaled=0.000003, accumulated=0.004672
âŒ Error in batch 3: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 254.88 MiB is free. Including non-PyTorch memory, this process has 78.85 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 367.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 5/64: loss=0.010442, scaled=0.002610, accumulated=0.007282
âŒ Error in batch 4: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 24.88 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 327.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 6/64: loss=0.000013, scaled=0.000003, accumulated=0.007285
âŒ Error in batch 5: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 206.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 415.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 4.Victim with financial loss
| 4.Victim with fi...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 7/64: loss=0.061617, scaled=0.015404, accumulated=0.022690
âŒ Error in batch 6: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 8/64: loss=0.010442, scaled=0.002610, accumulated=0.025300
âŒ Error in batch 7: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 9/64: loss=0.013093, scaled=0.003273, accumulated=0.028573
âŒ Error in batch 8: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 10/64: loss=0.013093, scaled=0.003273, accumulated=0.031846
âŒ Error in batch 9: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 11/64: loss=0.017710, scaled=0.004427, accumulated=0.036274
âŒ Error in batch 10: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 12/64: loss=0.000013, scaled=0.000003, accumulated=0.036277
âŒ Error in batch 11: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 13/64: loss=0.010442, scaled=0.002610, accumulated=0.038887
âŒ Error in batch 12: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 14/64: loss=0.010442, scaled=0.002610, accumulated=0.041498
âŒ Error in batch 13: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 15/64: loss=0.010442, scaled=0.002610, accumulated=0.044108
âŒ Error in batch 14: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 16/64: loss=0.017710, scaled=0.004427, accumulated=0.048536
âŒ Error in batch 15: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 17/64: loss=0.017710, scaled=0.004427, accumulated=0.052963
âŒ Error in batch 16: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Tele...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Tel...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 18/64: loss=0.000950, scaled=0.000238, accumulated=0.053201
âŒ Error in batch 17: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 19/64: loss=0.010442, scaled=0.002610, accumulated=0.055811
âŒ Error in batch 18: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 20/64: loss=0.010442, scaled=0.002610, accumulated=0.058422
âŒ Error in batch 19: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 21/64: loss=0.017710, scaled=0.004427, accumulated=0.062849
âŒ Error in batch 20: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 3. HASE Suspect
| 3.HASE Suspect | Unnamed: 1 |...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 22/64: loss=0.061617, scaled=0.015404, accumulated=0.078253
âŒ Error in batch 21: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 23/64: loss=0.000013, scaled=0.000003, accumulated=0.078257
âŒ Error in batch 22: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 24/64: loss=0.013093, scaled=0.003273, accumulated=0.081530
âŒ Error in batch 23: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 25/64: loss=0.010442, scaled=0.002610, accumulated=0.084140
âŒ Error in batch 24: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 26/64: loss=0.000013, scaled=0.000003, accumulated=0.084143
âŒ Error in batch 25: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 27/64: loss=0.000013, scaled=0.000003, accumulated=0.084147
âŒ Error in batch 26: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 28/64: loss=0.017710, scaled=0.004427, accumulated=0.088574
âŒ Error in batch 27: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 29/64: loss=0.010442, scaled=0.002610, accumulated=0.091185
âŒ Error in batch 28: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 30/64: loss=0.013093, scaled=0.003273, accumulated=0.094458
âŒ Error in batch 29: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 31/64: loss=0.013093, scaled=0.003273, accumulated=0.097731
âŒ Error in batch 30: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Inve...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Inv...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 32/64: loss=0.000950, scaled=0.000238, accumulated=0.097968
âŒ Error in batch 31: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 33/64: loss=0.017710, scaled=0.004427, accumulated=0.102396
âŒ Error in batch 32: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 34/64: loss=0.017710, scaled=0.004427, accumulated=0.106823
âŒ Error in batch 33: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 35/64: loss=0.010442, scaled=0.002610, accumulated=0.109434
âŒ Error in batch 34: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 36/64: loss=0.017710, scaled=0.004427, accumulated=0.113861
âŒ Error in batch 35: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 37/64: loss=0.010442, scaled=0.002610, accumulated=0.116472
âŒ Error in batch 36: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 38/64: loss=0.010442, scaled=0.002610, accumulated=0.119082
âŒ Error in batch 37: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 4.Victim with financial loss
| 4.Victim with fi...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 39/64: loss=0.061617, scaled=0.015404, accumulated=0.134486
âŒ Error in batch 38: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Soci...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Rom...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 40/64: loss=0.000950, scaled=0.000238, accumulated=0.134724
âŒ Error in batch 39: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 41/64: loss=0.010442, scaled=0.002610, accumulated=0.137334
âŒ Error in batch 40: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 42/64: loss=0.013093, scaled=0.003273, accumulated=0.140607
âŒ Error in batch 41: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 43/64: loss=0.010442, scaled=0.002610, accumulated=0.143218
âŒ Error in batch 42: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 44/64: loss=0.010442, scaled=0.002610, accumulated=0.145828
âŒ Error in batch 43: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 45/64: loss=0.000013, scaled=0.000003, accumulated=0.145832
âŒ Error in batch 44: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 46/64: loss=0.017710, scaled=0.004427, accumulated=0.150259
âŒ Error in batch 45: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 47/64: loss=0.017710, scaled=0.004427, accumulated=0.154686
âŒ Error in batch 46: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Soci...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Soc...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 48/64: loss=0.000950, scaled=0.000238, accumulated=0.154924
âŒ Error in batch 47: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 49/64: loss=0.000013, scaled=0.000003, accumulated=0.154927
âŒ Error in batch 48: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 50/64: loss=0.010442, scaled=0.002610, accumulated=0.157538
âŒ Error in batch 49: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 51/64: loss=0.000013, scaled=0.000003, accumulated=0.157541
âŒ Error in batch 50: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 52/64: loss=0.000013, scaled=0.000003, accumulated=0.157544
âŒ Error in batch 51: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 5.Victim with financial loss
| 5.Victim with fi...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 53/64: loss=0.061617, scaled=0.015404, accumulated=0.172949
âŒ Error in batch 52: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 54/64: loss=0.000013, scaled=0.000003, accumulated=0.172952
âŒ Error in batch 53: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 55/64: loss=0.013093, scaled=0.003273, accumulated=0.176225
âŒ Error in batch 54: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 56/64: loss=0.010442, scaled=0.002610, accumulated=0.178835
âŒ Error in batch 55: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Victim ODFT case. E...'
  Input: '## 3. HASE Suspect
| 3.HASE Suspect | Unnamed: 1 |...'
  Output: '```json
{
  "source": "Victim ODFT",
  "fraud_type...'
  Full text: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Prompt only: 'Instruction: You are given the content of a Victim ODFT case. Extract and summarize the information ...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 57/64: loss=0.061617, scaled=0.015404, accumulated=0.194240
âŒ Error in batch 56: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 58/64: loss=0.000013, scaled=0.000003, accumulated=0.194243
âŒ Error in batch 57: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an Internal Referral ...'
  Input: 'FC-UAR document
Page x of y

URP-0000000001 - Orig...'
  Output: '```json
{
  "source": "Internal Referral",
  "frau...'
  Full text: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Prompt only: 'Instruction: You are given the content of an Internal Referral case. Extract and summarize the infor...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 59/64: loss=0.017710, scaled=0.004427, accumulated=0.198670
âŒ Error in batch 58: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Search Warrant case...'
  Input: 'Police Force Ordinance
(Cap. 232)

Contact Person:...'
  Output: '```json
{
  "source": "Search Warrant",
  "fraud_t...'
  Full text: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of a Search Warrant case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 60/64: loss=0.013093, scaled=0.003273, accumulated=0.201943
âŒ Error in batch 59: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 61/64: loss=0.000013, scaled=0.000003, accumulated=0.201947
âŒ Error in batch 60: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of a Police Letter case....'
  Input: '**<u>Annex H6</u>**
**PERSONAL DATA**
*Our Ref.:* ...'
  Output: '```json
{
  "source": "Police Letter",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Prompt only: 'Instruction: You are given the content of a Police Letter case. Extract and summarize the informatio...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 62/64: loss=0.000013, scaled=0.000003, accumulated=0.201950
âŒ Error in batch 61: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an ADCC case. Extract...'
  Input: 'Dear HSB,

Please be informed of a case of **"Inve...'
  Output: '```json
{
  "source": "ADCC",
  "fraud_type": "Inv...'
  Full text: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Prompt only: 'Instruction: You are given the content of an ADCC case. Extract and summarize the information into J...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 63/64: loss=0.000950, scaled=0.000238, accumulated=0.202188
âŒ Error in batch 62: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 22.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.03 GiB is allocated by PyTorch, and 329.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ” Processing item 1:
  Type: <class 'dict'>
  Keys: ['Instruction', 'Input', 'Output']
  Instruction: 'You are given the content of an HSBC Referral case...'
  Input: 'Dear team,

There is a fraud case reporting involv...'
  Output: '```json
{
  "source": "HSBC Referral",
  "fraud_ty...'
  Full text: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Prompt only: 'Instruction: You are given the content of an HSBC Referral case. Extract and summarize the informati...'
  Kept last 5 tokens as labels
  Final sequence length: 512
  Valid labels: 5
âœ… Successfully processed 1 items in batch
ğŸ“Š Final batch shape: torch.Size([1, 512])
ğŸ“Š Token ID range: 1 to 94506
ğŸ”„ Batch 64/64: loss=0.010442, scaled=0.002610, accumulated=0.204798
âŒ Error in batch 63: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 204.88 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 417.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

âœ… Epoch 3 completed:
    Average loss: 0.003200
    Duration: 58.8s
    Optimization steps this epoch: -32
    Total optimization steps so far: 0

ğŸ‰ Training completed! Total optimization steps: 0
ğŸš¨ WARNING: No optimization steps were performed!
ğŸš¨ The model was not actually trained!

==================================================
âŒ MODEL PARALLEL TRAINING FAILED!
==================================================

==================================================
STARTING CLEANUP PROCESS
==================================================
Closing TensorBoard writer...
Clearing model...
Cleanup completed!
==================================================
ğŸ§¹ All cleanup completed. Exiting.

Program exiting normally...

==================================================
STARTING CLEANUP PROCESS
==================================================
Cleanup completed!
==================================================
Found 0 child processes to terminate
